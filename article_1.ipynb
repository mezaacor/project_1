{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting, Methods and Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ronny A. Meza A. and Alberto Fernández H**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the complement of an article with the same name. In this notebook we are to share the data and methodology used for to reach the goals. The main objective will be to evaluate the evolution of the time series forecasting methods and the different topics to which it was applied over time. The main objective will be to evaluate the evolution of the time series forecasting methods and the different topics to which it was applied over time. In this way, we downloaded the references from Web of Sciences, with the following search criterias: Query: (TS=(time* AND series* AND forecasting)) AND LANGUAGE: (English) AND DOCUMENT TYPES: (Article OR Book Chapter OR Data Paper OR Database Review). The references with the all fields were downloaded in batches of 500 until until completing all the references of the search. For more information about getting bibliography metadata consult the following link: Bibliographyc Metadata.\n",
    "\n",
    "The analysis was developed using Tethne Python package. Tethne is a Python software package for parsing and analyzing bibliographic metadata; This tool was developed by the Laubichler Lab and the Digital Innovation Group at Arizona State University as part of an initiative for digital and computational humanities (d+cH). It is important to take in consideration that Tethne is developed in Python 2.7. Python 3 is a distinct language from Python 2.7, and many packages have not yet made the leap to this new platform. A Python 3-compatible version is in the works.\n",
    "\n",
    "In this project we use Tethne because, it provides tools for easily parsing and analyzing bibliographic data in Python. The primary emphasis is on working with data from the Web of Science database, and also, Tethne provides efficient methods for modeling and analyzing topics changing in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "#%reset\n",
    "%pylab inline\n",
    "from tethne.readers import wos\n",
    "from tethne.readers import merge\n",
    "import matplotlib.pyplot as plt\n",
    "from tethne import networks\n",
    "from tethne.writers.graph import to_graphml\n",
    "from tethne import bibliographic_coupling\n",
    "from tethne.networks import authors\n",
    "from tethne.utilities import _iterable\n",
    "from tethne import GraphCollection\n",
    "from tethne.networks import coauthors\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from tethne import networks\n",
    "from pprint import pprint\n",
    "from tethne import tokenize\n",
    "from tethne.model.corpus import mallet\n",
    "from tethne.networks import topics\n",
    "from tethne.writers import graph\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tethne.analyze.corpus import burstness\n",
    "from tethne.plot import plot_burstness\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_corpus = wos.read('/home/ronny/Documentos/Artículos Doctorado/jupyter_codes/WOS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "References from **Web of Sciences** with the following search parameters: *(TS=(time* AND series* AND forecasting)) AND LANGUAGE: (English) AND DOCUMENT TYPES: (Article OR Book Chapter OR Data Paper OR Database Review).* `The time windows is from 1900 to march, 15, 2019`. \n",
    "\n",
    "The Tethne' library called **read** can to load a list of Papers from a directory containing multiple data files; in our case, we have 31 records. Only it is necesary to provide the path to a directory containing several WoS field-tagged data files. The read function knows that your path is a directory and not a data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_corpus = wos.read('/home/ronny/Documentos/Artículos Doctorado/Article_1_py/WOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_num = len(wos_corpus)\n",
    "print 'Loaded %i Web of Sciences references' % ref_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating graphics with Latex properties\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "## for Palatino and other serif fonts use:\n",
    "#rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.unicode'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we can look how documents themselves are distributed using the distribution() method.\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"]= [6.4, 4]\n",
    "plt.rcParams[\"figure.figsize\"]= [16, 6]\n",
    "\n",
    "plt.bar(*wos_corpus.distribution(), color='gray', edgecolor='black')    # <-- The action.\n",
    "\n",
    "plt.xlabel(r'\\textbf{YEAR}', fontsize=12)\n",
    "plt.ylabel(r'\\textbf{NUMBER OF DOCUMENTS}', fontsize=12)\n",
    "plt.title(r'\\textbf{TIME SERIES FORECASTING PUBLICATIONS PER YEAR}', fontsize=14, color='k')\n",
    "plt.grid(True)\n",
    "#plt.savefig('figure_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 WoS Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract_to_features method converts all of the available abstracts in our Corpus to a unigram featureset. It takes no arguments. The abstracts will be diced up into their constituent words, punctuation and capitalization is removed, and a featureset called abstractTerms is generated. By default, abstract_to_features will apply the NLTK stoplist and Porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_corpus.index_feature('abstract', tokenize=tokenize, structured=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_corpus.features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'There are {0} features in the abstract featureset.'.format(len(wos_corpus.features['abstract'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Only {0} of {1} papers have abstracts'.format(len(wos_corpus.features['abstract'].features), len(wos_corpus.papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'This corpus contains the following features: \\n\\t%s' % '\\n\\t'.join(wos_corpus.features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing research topics in time series forecasting, each 10 years \n",
    "**Dividing the Population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_1940 = wos_corpus.subcorpus(('date', range(1940, 1990)))\n",
    "corpus_1990 = wos_corpus.subcorpus(('date', range(1991, 2000)))\n",
    "corpus_2000 = wos_corpus.subcorpus(('date', range(2001, 2010)))\n",
    "corpus_2010 = wos_corpus.subcorpus(('date', range(2011, 2020)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for determinant the corpus length\n",
    "def len_corpus(corpus_data):\n",
    "    ref_num1 = len(corpus_data)\n",
    "    print 'The subcorpus has %i references' % ref_num1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_corpus(corpus_1940)\n",
    "len_corpus(corpus_1990)\n",
    "len_corpus(corpus_2000)\n",
    "len_corpus(corpus_2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subcorpus 1\n",
    "plt.bar(*corpus_1940.distribution(), color='gray', edgecolor='black')    # <-- The action.\n",
    "plt.xlabel(r'\\textbf{YEAR}', fontsize=11)\n",
    "plt.ylabel(r'\\textbf{NUMBER OF DOCUMENTS}', fontsize=11)\n",
    "plt.title(r'\\textbf{PUBLICATIONS (1940 - 1990)}', fontsize = 14, color='k')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subcorpus 2\n",
    "plt.bar(*corpus_1990.distribution(), color='gray', edgecolor='black')    # <-- The action.\n",
    "plt.xlabel(r'\\textbf{YEAR}', fontsize=11)\n",
    "plt.ylabel(r'\\textbf{NUMBER OF DOCUMENTS}', fontsize=11)\n",
    "plt.title(r'\\textbf{PUBLICATIONS (1990 - 2000)}', fontsize = 14, color='k')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subcorpus 3\n",
    "plt.bar(*corpus_2000.distribution(), color='gray', edgecolor='black')    # <-- The action.\n",
    "plt.xlabel(r'\\textbf{YEAR}', fontsize=11)\n",
    "plt.ylabel(r'\\textbf{NUMBER OF DOCUMENTS}', fontsize=11)\n",
    "plt.title(r'\\textbf{PUBLICATIONS (2000 - 2010)}', fontsize = 14, color='k')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subcorpus 4\n",
    "plt.bar(*corpus_2010.distribution(), color='gray', edgecolor='black')    # <-- The action.\n",
    "plt.xlabel(r'\\textbf{YEAR}', fontsize=11)\n",
    "plt.ylabel(r'\\textbf{NUMBER OF DOCUMENTS}', fontsize=11)\n",
    "plt.title(r'\\textbf{PUBLICATIONS (2010 - 2019)}', fontsize = 14, color='k')\n",
    "plt.grid(True)\n",
    "#plt.savefig('subcorpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subcorpus Keywords Analysis (1940-1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wos_corpus.index('authorKeywords')\n",
    "\n",
    "corpus_1940.index_feature('authorKeywords')\n",
    "corpus_1940.features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = list()\n",
    "featureset = corpus_1940.features['authorKeywords']\n",
    "for k, count in featureset.documentCounts.items():\n",
    "    store.append((featureset.index[k], count))\n",
    "\n",
    "Keywords_1940 = sorted(store, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(Keywords_1940[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for search element inside the tuple and show in reverse order\n",
    "def search_tuple(data, element):\n",
    "    index = next((i for i,v in enumerate(data) if v[0] == element),-1)\n",
    "    return(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching Time Series Forecasting Method' indexes\n",
    "winter = search_tuple(Keywords_1940, 'HOLT-WINTERS')\n",
    "winter2 = search_tuple(Keywords_1940, 'HOLT\\u2010WINTERS')\n",
    "\n",
    "box1 = search_tuple(Keywords_1940, 'BOX-JENKINS')\n",
    "box2 = search_tuple(Keywords_1940, 'BOX\\u2010JENKINS')\n",
    "box3 = search_tuple(Keywords_1940, 'BOX-JENKINS MODELS')\n",
    "box4 = search_tuple(Keywords_1940, 'BOX-JENKINS ANALYSIS')\n",
    "\n",
    "arma1 = search_tuple(Keywords_1940, 'ARMA')\n",
    "arma2 = search_tuple(Keywords_1940, 'IARMA')\n",
    "arma3 = search_tuple(Keywords_1940, 'ARARMA MODEL')\n",
    "\n",
    "arima1 = search_tuple(Keywords_1940, 'ARIMA MODELS')\n",
    "arima2 = search_tuple(Keywords_1940, 'ARIMA')\n",
    "arima3 = search_tuple(Keywords_1940, 'ARIMA MODEL')\n",
    "arima4 = search_tuple(Keywords_1940, 'MULTIVARIATE ARIMA MODELS')\n",
    "\n",
    "smooth = search_tuple(Keywords_1940, 'EXPONENTIAL SMOOTHING')\n",
    "\n",
    "stochast = search_tuple(Keywords_1940, 'STOCHASTIC')\n",
    "\n",
    "bayes = search_tuple(Keywords_1940, 'BAYESIAN')\n",
    "\n",
    "regress1 = search_tuple(Keywords_1940, 'REGRESSION')\n",
    "regress2 = search_tuple(Keywords_1940, 'REGRESSION ANALYSIS')\n",
    "regress3 = search_tuple(Keywords_1940, 'ROBUST REGRESSION')\n",
    "regress4 = search_tuple(Keywords_1940, 'REGRESSION, DYNAMIC REGRESSION')\n",
    "regress5 = search_tuple(Keywords_1940, 'TIME-SERIES REGRESSION')\n",
    "\n",
    "kalman = search_tuple(Keywords_1940, 'KALMAN FILTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unification list for different forecasting methods\n",
    "\n",
    "#HOLT-WINTERS\n",
    "winter_id = Keywords_1940[winter][0]\n",
    "winter_ref = Keywords_1940[winter][1] + Keywords_1940[winter2][1]\n",
    "winter_ind = (winter_id, winter_ref)\n",
    "\n",
    "#BOX-JENKINS\n",
    "box_id = Keywords_1940[box1][0]\n",
    "box_ref = Keywords_1940[box1][1] + Keywords_1940[box2][1] + Keywords_1940[box3][1] + Keywords_1940[box4][1]\n",
    "box_ind = (box_id, box_ref)\n",
    "\n",
    "#ARMA\n",
    "arma_id = Keywords_1940[arma1][0]\n",
    "arma_ref = Keywords_1940[arma1][1] + Keywords_1940[arma2][1] + Keywords_1940[arma3][1]\n",
    "arma_ind = (arma_id, arma_ref)\n",
    "\n",
    "#ARIMA\n",
    "arima_id = Keywords_1940[arima2][0]\n",
    "arima_ref = Keywords_1940[arima1][1] + Keywords_1940[arima2][1] + Keywords_1940[arima3][1] + Keywords_1940[arima4][1]\n",
    "arima_ind = (arima_id, arima_ref)\n",
    "\n",
    "#REGRESSION\n",
    "regress_id = Keywords_1940[regress1][0]\n",
    "regress_ref = Keywords_1940[regress1][1] + Keywords_1940[regress2][1] + Keywords_1940[regress3][1] + Keywords_1940[regress4][1] + Keywords_1940[regress5][1]\n",
    "regress_ind = (regress_id, regress_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph for methods\n",
    "\n",
    "methods = [Keywords_1940[stochast], Keywords_1940[bayes], Keywords_1940[smooth],\n",
    "           Keywords_1940[kalman], winter_ind, box_ind, arma_ind, arima_ind, regress_ind]\n",
    "\n",
    "# sort in-place from highest to lowest\n",
    "methods.sort(key=lambda x: x[1], reverse=False) \n",
    "\n",
    "# save the names and their respective scores separately\n",
    "# reverse the tuples to go from most frequent to least frequent \n",
    "topic = zip(*methods)[0]\n",
    "score = zip(*methods)[1]\n",
    "\n",
    "x_pos = np.arange(len(topic)) \n",
    "\n",
    "plt.barh(x_pos, score, align='center', color='gray', edgecolor='black')\n",
    "plt.yticks(x_pos, topic)\n",
    "plt.xlabel(r'\\textbf{NUMBER OF DOCUMENTS WHERE THE METHOD WAS MENTIONED}')\n",
    "plt.ylabel(r'\\textbf{METHODS}')\n",
    "plt.title(r'\\textbf{SOME FORECASTING METHODS THAT HAVE BEEN USED BETWEEN 1940-1990}', fontsize=14, color='k')\n",
    "\n",
    "plt.grid(True)\n",
    "#plt.savefig('figure_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching Research Topics index\n",
    "#ECONOMIC\n",
    "economic1 = search_tuple(Keywords_1940, 'ECONOMIC')\n",
    "economic2 = search_tuple(Keywords_1940, 'ECONOMETRIC MODELS')\n",
    "economic3 = search_tuple(Keywords_1940, 'ECONOMETRIC FORECASTING')\n",
    "economic4 = search_tuple(Keywords_1940, 'ECONOMETRIC MODEL')\n",
    "economic5 = search_tuple(Keywords_1940, 'ECONOMIC TIME SERIES')\n",
    "\n",
    "#SALES\n",
    "sales1 = search_tuple(Keywords_1940, 'SALES')\n",
    "sales2 = search_tuple(Keywords_1940, 'SALES FORECASTING')\n",
    "\n",
    "#ENERGY\n",
    "energy1 = search_tuple(Keywords_1940, 'ENERGY')\n",
    "energy2 = search_tuple(Keywords_1940, 'POWER')\n",
    "\n",
    "demand = search_tuple(Keywords_1940, 'DEMAND')\n",
    "\n",
    "weather = search_tuple(Keywords_1940, 'WEATHER')\n",
    "\n",
    "market = search_tuple(Keywords_1940, 'MARKET')\n",
    "\n",
    "finance = search_tuple(Keywords_1940, 'FINANCIAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unification list for different forecasting methods\n",
    "\n",
    "#ECONOMIC\n",
    "economic_id = Keywords_1940[economic1][0]\n",
    "economic_ref = Keywords_1940[economic1][1] + Keywords_1940[economic2][1] + Keywords_1940[economic3][1] + Keywords_1940[economic4][1] + Keywords_1940[economic5][1]\n",
    "economic_ind = (economic_id, economic_ref)\n",
    "\n",
    "#SALES\n",
    "sales_id = Keywords_1940[sales1][0]\n",
    "sales_ref = Keywords_1940[sales1][1] + Keywords_1940[sales2][1]\n",
    "sales_ind = (sales_id, sales_ref)\n",
    "\n",
    "#ENERGY\n",
    "energy_id = Keywords_1940[energy1][0]\n",
    "energy_ref = Keywords_1940[energy1][1] + Keywords_1940[energy2][1]\n",
    "energy_ind = (energy_id, energy_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph for topics\n",
    "\n",
    "techniques = [Keywords_1940[demand], Keywords_1940[weather], Keywords_1940[market], Keywords_1940[finance],\n",
    "           economic_ind, sales_ind, energy_ind]\n",
    "\n",
    "# sort in-place from highest to lowest\n",
    "techniques.sort(key=lambda x: x[1], reverse=False) \n",
    "\n",
    "# save the names and their respective scores separately\n",
    "# reverse the tuples to go from most frequent to least frequent \n",
    "technique = zip(*techniques)[0]\n",
    "score = zip(*techniques)[1]\n",
    "#print(people)\n",
    "x_pos = np.arange(len(technique)) \n",
    "   \n",
    "plt.barh(x_pos, score, align='center', color='gray', edgecolor='black')\n",
    "plt.yticks(x_pos, technique)\n",
    "plt.xlabel(r'\\textbf{NUMBER OF DOCUMENTS WHERE THE TOPIC WAS MENTIONED}')\n",
    "plt.ylabel(r'\\textbf{TECHNIQUES USED}')\n",
    "plt.title(r'\\textbf{SOME TOPICS THAT HAVE BEEN STUDIED BETWEEN 1940-1990}', fontsize=14, color='k')\n",
    "\n",
    "plt.grid(True)\n",
    "#plt.savefig('figure_3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subcorpus Keywords Analysis (1991-2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wos_corpus.index('authorKeywords')\n",
    "\n",
    "corpus_1990.index_feature('authorKeywords')\n",
    "corpus_1990.features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = list()\n",
    "featureset = corpus_1990.features['authorKeywords']\n",
    "for k, count in featureset.documentCounts.items():\n",
    "    store.append((featureset.index[k], count))\n",
    "\n",
    "Keywords_1990 = sorted(store, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(Keywords_1990[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching Time Series Forecasting Methods\n",
    "\n",
    "#REGRESSION\n",
    "regress1 = search_tuple(Keywords_1990, 'REGRESSION')\n",
    "regress2 = search_tuple(Keywords_1990, 'TIME-SERIES REGRESSION')\n",
    "regress3 = search_tuple(Keywords_1990, 'LINEAR-REGRESSION')\n",
    "regress4 = search_tuple(Keywords_1990, 'INTERPOLATION')\n",
    "regress5 = search_tuple(Keywords_1990, 'EXTRAPOLATION')\n",
    "\n",
    "#AUTOREGRESSIVE\n",
    "arima1 = search_tuple(Keywords_1990, 'AUTOREGRESSIVE TIME-SERIES')\n",
    "arima2 = search_tuple(Keywords_1990, 'ARIMA')\n",
    "\n",
    "#HETEROSCEDASTICITY\n",
    "hetero1 = search_tuple(Keywords_1990, 'HETEROSKEDASTICITY')\n",
    "hetero2 = search_tuple(Keywords_1990, 'CONDITIONAL HETEROSKEDASTICITY')\n",
    "hetero3 = search_tuple(Keywords_1990, 'HETEROSCEDASTICITY')\n",
    "\n",
    "#NEURAL NETWORKS\n",
    "neural1 = search_tuple(Keywords_1990, 'NEURAL NETWORKS')\n",
    "neural2 = search_tuple(Keywords_1990, 'MULTILAYER FEEDFORWARD NETWORKS')\n",
    "\n",
    "lyapunov = search_tuple(Keywords_1990, 'LYAPUNOV EXPONENTS')\n",
    "\n",
    "simu = search_tuple(Keywords_1990, 'SIMULATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unification list for different forecasting methods\n",
    "\n",
    "#REGRESSION\n",
    "regress_id = Keywords_1990[regress1][0]\n",
    "regress_ref = Keywords_1990[regress1][1] + Keywords_1990[regress2][1] + Keywords_1990[regress3][1] + Keywords_1990[regress4][1] + Keywords_1990[regress5][1]\n",
    "regress_ind = (regress_id, regress_ref)\n",
    "\n",
    "\n",
    "#AUTOREGRESSIVE\n",
    "arima_id = Keywords_1990[arima2][0]\n",
    "arima_ref = Keywords_1990[arima1][1] + Keywords_1990[arima2][1]\n",
    "arima_ind = (arima_id, arima_ref)\n",
    "\n",
    "#HETEROSCEDASTICITY\n",
    "hetero_id = Keywords_1990[hetero1][0]\n",
    "hetero_ref = Keywords_1990[hetero1][1] + Keywords_1990[hetero2][1] + Keywords_1990[hetero3][1]\n",
    "hetero_ind = (hetero_id, hetero_ref)\n",
    "\n",
    "#NEURAL NETWORKS\n",
    "neural_id = Keywords_1990[neural1][0]\n",
    "neural_ref = Keywords_1990[neural1][1] + Keywords_1990[neural2][1]\n",
    "neural_ind = (neural_id, neural_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph for topics\n",
    "topics = [Keywords_1990[lyapunov], Keywords_1990[simu], regress_ind, arima_ind, hetero_ind, neural_ind]\n",
    "\n",
    "# sort in-place from highest to lowest\n",
    "topics.sort(key=lambda x: x[1], reverse=False) \n",
    "\n",
    "# reverse the tuples to go from most frequent to least frequent \n",
    "topic = zip(*topics)[0]\n",
    "score = zip(*topics)[1]\n",
    "\n",
    "x_pos = np.arange(len(topics)) \n",
    "  \n",
    "plt.barh(x_pos, score, align='center', color='gray', edgecolor='black')\n",
    "plt.yticks(x_pos, topic)\n",
    "plt.xlabel(r'\\textbf{NUMBER OF DOCUMENTS WHERE THE TECHNIQUE WAS MENTIONED}')\n",
    "plt.ylabel(r'\\textbf{METHODS}')\n",
    "plt.title(r'\\textbf{SOME TOPICS THAT HAVE BEEN STUDIED BETWEEN 1990-2000}', fontsize=14, color='k')\n",
    "\n",
    "plt.grid(True)\n",
    "#plt.savefig('figure_3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching Time Series Forecasting Topics\n",
    "\n",
    "#DEMAND\n",
    "demand = search_tuple(Keywords_1990, 'DEMAND')\n",
    "\n",
    "#RETURNS\n",
    "returns1 = search_tuple(Keywords_1990, 'STOCK RETURNS')\n",
    "returns2 = search_tuple(Keywords_1990, 'RETURNS')\n",
    "\n",
    "#MARKET\n",
    "market1 = search_tuple(Keywords_1990, 'MARKET')\n",
    "market2 = search_tuple(Keywords_1990, 'MARKETS')\n",
    "\n",
    "#ECONOMIC\n",
    "economic1 = search_tuple(Keywords_1990, 'ECONOMIC TIME-SERIES')\n",
    "economic2 = search_tuple(Keywords_1990, 'MACROECONOMIC TIME-SERIES')\n",
    "economic3 = search_tuple(Keywords_1990, 'ECONOMIC FORECASTS')\n",
    "economic4 = search_tuple(Keywords_1990, 'ECONOMY')\n",
    "economic5 = search_tuple(Keywords_1990, 'INFLATION')\n",
    "economic6 = search_tuple(Keywords_1990, 'INTEREST-RATES')\n",
    "economic7 = search_tuple(Keywords_1990, 'RATES')\n",
    "\n",
    "#WEATHER\n",
    "weather1 = search_tuple(Keywords_1990, 'PRECIPITATION')\n",
    "weather2 = search_tuple(Keywords_1990, 'CLIMATE')\n",
    "weather3 = search_tuple(Keywords_1990, 'SEA-SURFACE TEMPERATURE')\n",
    "weather4 = search_tuple(Keywords_1990, 'TEMPERATURES')\n",
    "weather5 = search_tuple(Keywords_1990, 'WEATHER')\n",
    "weather6 = search_tuple(Keywords_1990, 'WEATHER PREDICTION')\n",
    "weather7 = search_tuple(Keywords_1990, 'SURFACE-TEMPERATURE')\n",
    "weather8 = search_tuple(Keywords_1990, 'TEMPERATURE')\n",
    "weather9 = search_tuple(Keywords_1990, 'RAINFALL')\n",
    "\n",
    "#FINANCES\n",
    "finances1 = search_tuple(Keywords_1990, 'VOLATILITY')\n",
    "finances2 = search_tuple(Keywords_1990, 'STOCHASTIC VOLATILITY')                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unification list for different forecasting topics\n",
    "\n",
    "#RETURNS\n",
    "returns_id = Keywords_1990[returns1][0]\n",
    "returns_ref = Keywords_1990[returns1][1] + Keywords_1990[returns2][1]\n",
    "returns_ind = (returns_id, returns_ref)\n",
    "\n",
    "#MARKET\n",
    "market_id = Keywords_1990[market2][0]\n",
    "market_ref = Keywords_1990[market1][1] + Keywords_1990[market2][1]\n",
    "market_ind = (market_id, market_ref)\n",
    "\n",
    "#ECONOMIC\n",
    "economic_id = Keywords_1990[economic4][0]\n",
    "economic_ref = Keywords_1990[economic1][1] + Keywords_1990[economic2][1] + Keywords_1990[economic3][1] + Keywords_1990[economic4][1] + Keywords_1990[economic5][1] + Keywords_1990[economic6][1] + Keywords_1990[economic7][1]\n",
    "economic_ind = (economic_id, economic_ref)\n",
    "\n",
    "#WEATHER\n",
    "weather_id = Keywords_1990[weather5][0]\n",
    "weather_ref = Keywords_1990[weather1][1] + Keywords_1990[weather2][1] + Keywords_1990[weather3][1] + Keywords_1990[weather4][1] + Keywords_1990[weather5][1] + Keywords_1990[weather6][1] + Keywords_1990[weather7][1] + Keywords_1990[weather8][1] + + Keywords_1990[weather9][1]\n",
    "weather_ind = (weather_id, weather_ref)\n",
    "\n",
    "finances_id = Keywords_1990[finances1][0]\n",
    "finances_ref = Keywords_1990[finances1][1] + Keywords_1990[finances2][1]\n",
    "finances_ind = (finances_id, finances_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar graph for topics\n",
    "\n",
    "techniques = [Keywords_1990[demand], returns_ind, market_ind, economic_ind, weather_ind, finances_ind]\n",
    "\n",
    "# sort in-place from highest to lowest\n",
    "techniques.sort(key=lambda x: x[1], reverse=False) \n",
    "\n",
    "# save the names and their respective scores separately\n",
    "# reverse the tuples to go from most frequent to least frequent \n",
    "technique = zip(*techniques)[0]\n",
    "score = zip(*techniques)[1]\n",
    "\n",
    "x_pos = np.arange(len(technique)) \n",
    "   \n",
    "plt.barh(x_pos, score, align='center', color='gray', edgecolor='black')\n",
    "plt.yticks(x_pos, technique)\n",
    "plt.xlabel(r'\\textbf{NUMBER OF DOCUMENTS WHERE THE TOPIC WAS MENTIONED}')\n",
    "plt.ylabel(r'\\textbf{TECHNIQUES USED}')\n",
    "plt.title(r'\\textbf{SOME TOPICS THAT HAVE BEEN STUDIED BETWEEN 1991-2000}', fontsize=14, color='k')\n",
    "\n",
    "plt.grid(True)\n",
    "#plt.savefig('figure_3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter = search_tuple(Keywords_1990, 'HOLT-WINTERS')\n",
    "winter = search_tuple(Keywords_1990, 'HOLT-WINTERS')\n",
    "winter = search_tuple(Keywords_1990, 'HOLT-WINTERS')\n",
    "winter = search_tuple(Keywords_1990, 'HOLT-WINTERS')\n",
    "winter = search_tuple(Keywords_1990, 'HOLT-WINTERS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
